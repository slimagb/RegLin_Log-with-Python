{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **PREPROCESSING**\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les libraries et le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/zefer/OneDrive/Bureau/Master II/Machine Learning/Exam/credit.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion de format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les types de variables au bon format\n",
    "\n",
    "data['Own'] = data['Own'].astype('category')\n",
    "data['Student'] = data['Student'].astype('category')\n",
    "data['Married'] = data['Married'].astype('category')\n",
    "data['Region'] = data['Region'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Own'].value_counts())\n",
    "print(data['Student'].value_counts())\n",
    "print(data['Married'].value_counts())\n",
    "print(data['Region'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodage des variables categorielles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des variables categorielles \n",
    "\n",
    "data['Own'].replace(['Yes', 'No'], [0,1], inplace=True)\n",
    "data['Student'].replace(['Yes', 'No'], [0,1], inplace=True)\n",
    "data['Married'].replace(['Yes', 'No'], [0,1], inplace=True)\n",
    "data['Region'].replace(['South', 'West', 'East'], [0,1, 2], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1) Fonction regression(X, Y) qui renvoie l’estimateur des moindre carrés.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affectation des variables explicatives a X et la variablle endogene a y et redimensionnement sous forme matricielle\n",
    "\n",
    "Y = data['Balance']\n",
    "X = data.drop('Balance', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\beta} = (X^{T}X)^{-1}X^{T}X y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(X, Y):\n",
    "    X = pd.concat([X, pd.DataFrame(np.ones((X.shape[0],1)))], axis=1)\n",
    "    XTX = X.T.dot(X)\n",
    "    beta = np.linalg.pinv(XTX).dot(X.T).dot(Y)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison avec un modele lineaire classique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modele\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "\n",
    "#Intercept and Coefficient\n",
    "\n",
    "beta = lr.intercept_\n",
    "alpha = lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interception est \", beta)\n",
    "print(\"Les coefficients sont \", alpha)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Fonction regress(X, α, β) qui renvoie le vecteur Y-chapeau\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y_i}=(\\alpha, x_i)+\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress(X, alpha, beta):\n",
    "    Y_chapeau = np.array(X.dot(alpha.T)+beta)\n",
    "    return Y_chapeau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress(X, alpha, beta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Calculer l’erreur au sens des moindres carrés du régresseur\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{errors} = \\sum_{i=1}^{n}(y_i-f(x_i))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère l'erreur de norme 2 sur le jeu de données test comme baseline\n",
    "error = np.sum((lr.predict(X) - Y) ** 2)\n",
    "\n",
    "print(error)\n",
    "\n",
    "#ou\n",
    "\n",
    "Y_chapeau = np.array(X.dot(alpha.T)+beta)\n",
    "\n",
    "def error(Y, Y_chapeau):\n",
    "    return np.sum((Y - Y_chapeau)**2) # mean ou sum\n",
    "\n",
    "error(Y, Y_chapeau)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Fonction ridge_regression(X, Y , lambda) qui renvoie l’estimateur des moindre carrés généralisés"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\frac{\\hat{\\alpha}}{\\hat{\\beta}})=(X^{T}X+\\lambda I_{p+1})^{-1}X^{T}Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(X.shape[1]) # Matrice identite d'ordre p+1\n",
    "\n",
    "def ridge_regression(X,Y,lambda_):\n",
    "    return np.dot(np.dot(np.linalg.pinv(np.dot(X.T,X)+(lambda_*I)), X.T), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 1\n",
    "ridge_regression(X, Y, lambda_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de ridge_regression classique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = linear_model.Ridge()\n",
    "rm.fit(X,Y)\n",
    "print(rm.coef_)\n",
    "print(rm.intercept_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "\n",
    " * On remarque que, tous les parametres de la regression ridge sont inferieur aux attributs(coef_ et intercep) d'un regresseur ridge de type classique \n",
    " \n",
    "*On peut donc conclure que la valeur de lambda diminue les poids de tous les paramètres de la régression ridge*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Tracez l’évolution des coefficients du vecteur alpha en fonction du paramètre de régularisation lambda pour des valeurs entre 0.001 et 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 200\n",
    "alphas = np.logspace(-2, 3, n_alphas)\n",
    "\n",
    "    \n",
    "coefs = []\n",
    "for l_ in alphas:  \n",
    "    ridge = linear_model.Ridge(alpha=l_, fit_intercept=False)\n",
    "    ridge.fit(X, Y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "coefs = np.array(coefs) \n",
    "coefs = coefs.reshape(coefs.shape[0], -1)\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.legend(labels = [])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  variables semblent le mieux expliquer la variable Balance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corr = d.corr()\n",
    "#Plot figsize\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#Generate Heat Map, allow annotations and place floats in map\n",
    "sns.heatmap(corr, cmap='RdBu', annot=True, fmt=\".2f\")\n",
    "#Apply xticks\n",
    "plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "#Apply yticks\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation :**\n",
    "\n",
    " * Les variables qui expliquent mieux la variable *Balance* sont : *Income, Limit, Cards, Age et Education*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) \n",
    "\n",
    "## - La meilleure valeur pour le paramètre lambda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*La meilleur de alpha est lorsque il est le plus bas, c'est a dire alpha minimal*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_cv = RidgeCV(alphas)\n",
    "\n",
    "# Entrainement du model\n",
    "model_cv = regr_cv.fit(X, Y) \n",
    "\n",
    "# alpha min\n",
    "model_cv.alpha_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Calculons l’erreur au sens des moindres carrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_chapeau = model_cv.predict(X) \n",
    "ridge_error = error(Y, Y_chapeau)\n",
    "ridge_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * L'erreur de la regression ridge est la meme que l'erreur au sens des moindres du regresseur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) En utilisant la classe linear_model.Lasso, tracez l’évolution des coefficients du vecteur αˆ en fonction de la valeur du paramètre lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 300\n",
    "alphas = np.logspace(-5, 1, n_alphas)\n",
    "lasso = linear_model.Lasso(fit_intercept=False)\n",
    "\n",
    "coefs = []\n",
    "errors = []\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X, Y)\n",
    "    coefs.append(lasso.coef_)\n",
    "    errors.append([error, np.mean((lasso.predict(X) - Y) ** 2)])\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('weights')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Les variables qui expliquent mieux la variable *Balance* sont : *Income, Limit, Cards, Age et Education*\n",
    "\n",
    " * Elle sont les memes que celle trouvee a la question precedente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) \n",
    "\n",
    "## - La meilleure valeur pour le paramètre lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_lasso = LassoCV(cv = 20 )\n",
    "\n",
    "# Entrement du modele\n",
    "model_lasso = regr_lasso.fit(X, Y) \n",
    "\n",
    "# Valeur d'alpha\n",
    "model_lasso.alpha_ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation :**\n",
    "\n",
    " * Comme on peut le voir, le lasso permet de supprimer des variables en mettant leur poids à zéro. C'est le cas si deux variables sont corrélées. L'une sera sélectionnée par le Lasso, l'autre supprimée"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Choix du modele approprie\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Dans les deux modeles( Ridge et Lasso), la régularisation fonctionne bien, et permet de réduire l'erreur totale du modèle à l'aide de l'ajout du biais qui quantifie la complexité. \n",
    " \n",
    "    * la regression Ridge diminue grandement l'influence de certaines variables sur le modèle\n",
    "    \n",
    "    * la regression Lasso peut directement supprimer l'influence de certaines variables sur le modele (mettre leur poids à zéro). \n",
    "\n",
    "*Les erreurs sont tres reduites dans la regression Lasso. Donc le Lasso est plus efficace, parcissiomieuse et approprie que le Ridge*\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
